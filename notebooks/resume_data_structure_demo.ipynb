{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Data Structure Demo\n",
    "\n",
    "This notebook demonstrates the `ResumeDocument`, `ResumeDocumentArchive` and `ResumeSection` classes from `archer.contexts.templating.resume_data_structure`.\n",
    "\n",
    "These classes provide a **simplified interface designed for the targeting context** to work with resume data by stripping away LaTeX formatting and layout metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from omegaconf import OmegaConf\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "from archer.contexts.templating.resume_data_structure import (\n",
    "    ResumeDocument,\n",
    "    ResumeDocumentArchive,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "RESUME_ARCHIVE_PATH = Path(os.getenv(\"RESUME_ARCHIVE_PATH\"))\n",
    "\n",
    "# Pick a sample resume\n",
    "sample_yaml = RESUME_ARCHIVE_PATH / \"structured\" / \"Res_ACMECorp.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Without ResumeDocument (OmegaConf)\n",
    "\n",
    "Let's see what working directly with the YAML looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with OmegaConf\n",
    "yaml_data = OmegaConf.load(sample_yaml)\n",
    "yaml_dict = OmegaConf.to_container(yaml_data, resolve=True)\n",
    "\n",
    "print(\"Top-level keys:\")\n",
    "print(list(yaml_dict.keys()))\n",
    "print(\"\\nDocument keys:\")\n",
    "print(list(yaml_dict[\"document\"].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Work Experience Directly from yaml (Without ResumeDocument)\n",
    "\n",
    "Notice how deeply nested and complex the structure is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate the complex nested structure\n",
    "doc = yaml_dict[\"document\"]\n",
    "\n",
    "# Find work experience sections\n",
    "work_experiences = []\n",
    "for page in doc.get(\"pages\", []):\n",
    "    for region_name, region_data in page.get(\"regions\", {}).items():\n",
    "        if region_data and \"sections\" in region_data:\n",
    "            for section in region_data[\"sections\"]:\n",
    "                # Check for nested subsections (work_history contains work_experience)\n",
    "                if \"subsections\" in section:\n",
    "                    for subsection in section[\"subsections\"]:\n",
    "                        if subsection.get(\"type\") == \"work_experience\":\n",
    "                            work_experiences.append(subsection)\n",
    "\n",
    "print(f\"Found {len(work_experiences)} work experiences\\n\")\n",
    "\n",
    "# Get first work experience\n",
    "first_work = work_experiences[0]\n",
    "print(\"Company:\", first_work[\"metadata\"][\"company\"])\n",
    "print(\"Title:\", first_work[\"metadata\"][\"title\"])\n",
    "print(\"Dates:\", first_work[\"metadata\"][\"dates\"])\n",
    "print(\"\\nProjects:\")\n",
    "for proj in first_work[\"content\"][\"projects\"][:2]:\n",
    "    # Note: project names still have LaTeX formatting\n",
    "    print(f\"  - {proj['metadata']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Skills (Without ResumeDocument)\n",
    "\n",
    "Also requires manual navigation and LaTeX cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find skill sections\n",
    "skill_sections = []\n",
    "for page in doc.get(\"pages\", []):\n",
    "    for region_name, region_data in page.get(\"regions\", {}).items():\n",
    "        if region_data and \"sections\" in region_data:\n",
    "            for section in region_data[\"sections\"]:\n",
    "                if section.get(\"type\") in (\"skill_list_caps\", \"skill_list_pipes\", \"skill_categories\"):\n",
    "                    skill_sections.append(section)\n",
    "\n",
    "print(f\"Found {len(skill_sections)} skill sections\\n\")\n",
    "\n",
    "# Get first skill section\n",
    "first_skills = skill_sections[0]\n",
    "print(f\"Section: {first_skills['metadata']['name']}\")\n",
    "print(\"Skills (raw - with LaTeX):\")\n",
    "for skill in first_skills[\"content\"][\"items\"][:5]:\n",
    "    print(f\"  - {skill}\")\n",
    "\n",
    "# Would need to manually clean LaTeX formatting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problems:\n",
    "\n",
    "1. **Complex navigation** - Multiple nested loops to find sections\n",
    "2. **Mixed with formatting** - Colors, spacing, LaTeX commands mixed with content\n",
    "3. **No text extraction** - Have to manually build plaintext\n",
    "4. **Lots of boilerplate** - Same navigation code repeated everywhere\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: ResumeDocument\n",
    "\n",
    "Now let's see the simplified interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with ResumeDocument\n",
    "doc = ResumeDocument(sample_yaml)\n",
    "\n",
    "print(f\"Name: {doc.name}\")\n",
    "print(f\"Title: {doc.professional_title}\")\n",
    "print(f\"Filename: {doc.filename}\")\n",
    "print(f\"Date: {doc.date}\")\n",
    "print(f\"Total sections: {len(doc.sections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "Get a formatted overview of all sections with one property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Work Experience (New Way)\n",
    "\n",
    "Work experiences are now grouped in `work_history` sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find work_history sections\n",
    "work_history_sections = [s for s in doc.sections if s.section_type == \"work_history\"]\n",
    "\n",
    "print(f\"Found {len(work_history_sections)} work history sections\\n\")\n",
    "\n",
    "# Get first work_history section\n",
    "first_history = work_history_sections[1]\n",
    "print(f\"Section: {first_history.name}\")\n",
    "print(f\"Number of work experiences: {len(first_history.data['subsections'])}\\n\")\n",
    "\n",
    "# Access first work experience within the section\n",
    "first_work = first_history.data['subsections'][0]\n",
    "\n",
    "# Structured data access\n",
    "print(f\"Company: {first_work['company']}\")\n",
    "print(f\"Title: {first_work['title']}\")\n",
    "print(f\"Dates: {first_work['dates']}\")\n",
    "print(f\"Location: {first_work['location']}\")\n",
    "\n",
    "# Show top-level bullets\n",
    "print(f\"\\nTop-level bullets ({len(first_work['items'])}):\\n\")\n",
    "for i, bullet in enumerate(first_work[\"items\"][:3], 1):\n",
    "    print(f\"{i}. {bullet[:100]}...\")\n",
    "if len(first_work[\"items\"]) > 3:\n",
    "    print(f\"   ... and {len(first_work['items']) - 3} more\")\n",
    "\n",
    "# Show projects\n",
    "print(f\"\\nProjects ({len(first_work['projects'])}):\\n\")\n",
    "\n",
    "for proj in first_work['projects']:\n",
    "    print(f\"üìå {proj['name']}\")\n",
    "    print(f\"   Bullets: {len(proj['items'])}\")\n",
    "    if proj['items']:\n",
    "        print(f\"   First bullet: {proj['items'][0][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Skills (using ResumeDocument)\n",
    "\n",
    "Clean, LaTeX-free text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find skill sections\n",
    "skill_sections = [\n",
    "    s for s in doc.sections \n",
    "    if s.section_type in (\"skill_list_caps\", \"skill_list_pipes\", \"skill_categories\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(skill_sections)} skill sections\\n\")\n",
    "\n",
    "for section in skill_sections:\n",
    "    print(f\"üìã {section.name} ({section.section_type}):\")\n",
    "    \n",
    "    # Handle flat skill lists (unified structure with \"items\")\n",
    "    if section.section_type in (\"skill_list_caps\", \"skill_list_pipes\"):\n",
    "        for skill in section.data['items'][:5]:\n",
    "            print(f\"   ‚Ä¢ {skill}\")\n",
    "        if len(section.data['items']) > 5:\n",
    "            print(f\"   ... and {len(section.data['items']) - 5} more\")\n",
    "    \n",
    "    # Handle categorized skills (conatainer with subsections)\n",
    "    elif section.section_type == \"skill_categories\":\n",
    "        for category in section.data['subsections'][:3]:  # Show first 3 categories\n",
    "            print(f\"   üìÅ {category['name']}:\")\n",
    "            for skill in category['items'][:3]:  # Show first 3 skills per category\n",
    "                print(f\"      ‚Ä¢ {skill}\")\n",
    "            if len(category['items']) > 3:\n",
    "                print(f\"      ... and {len(category['items']) - 3} more\")\n",
    "        if len(section.data['subsections']) > 3:\n",
    "            print(f\"   ... and {len(section.data['subsections']) - 3} more categories\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Manual iteration (preserves structure) vs get_items() (flattens)\n",
    "\n",
    "ml_tools = doc.get_section(\"ML Infrastructure Tools\")\n",
    "\n",
    "print(\"Manual iteration (structured):\")\n",
    "for category in ml_tools.data['subsections'][:2]:\n",
    "    print(f\"  üìÅ {category['name']}: {len(category['items'])} items\")\n",
    "\n",
    "print(\"\\nget_items() (flattened):\")\n",
    "all_tools = ml_tools.get_items()\n",
    "print(f\"  Total: {len(all_tools)} items (no category info)\")\n",
    "print(f\"  {all_tools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_items() works on all section types\n",
    "\n",
    "# Simple list (skill_list_caps)\n",
    "core_skills = doc.get_section(\"Core Skills\")\n",
    "print(f\"Core Skills: {len(core_skills.get_items())} items\")\n",
    "\n",
    "# Wrapper (skill_categories) - flattens all categories\n",
    "ml_tools = doc.get_section(\"ML Infrastructure Tools\")\n",
    "print(f\"ML Tools: {len(ml_tools.get_items())} items (across {len(ml_tools.data['subsections'])} categories)\")\n",
    "\n",
    "# Work history - flattens work bullets AND project bullets\n",
    "experience = doc.get_section(\"Experience\")\n",
    "print(f\"Experience: {len(experience.get_items())} bullets (work + projects combined)\")\n",
    "\n",
    "# Projects wrapper - flattens all projects\n",
    "projects = doc.get_section(\"Other Projects\")\n",
    "print(f\"Other Projects: {len(projects.get_items())} bullets (across {len(projects.data['subsections'])} projects)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case: Search for keyword across all work bullets\n",
    "experience = doc.get_section(\"Experience\")\n",
    "all_bullets = experience.get_items()\n",
    "\n",
    "keyword = \"GPU\"\n",
    "matches = [b for b in all_bullets if keyword.lower() in b.lower()]\n",
    "\n",
    "print(f\"Found {len(matches)} bullets mentioning '{keyword}':\\n\")\n",
    "for bullet in matches:\n",
    "    print(f\"  ‚Ä¢ {bullet[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_items_by_section() - Multiple Sections at Once\n",
    "\n",
    "For convenience, `ResumeDocument` has a method to get items from multiple sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get items from multiple skill sections\n",
    "items_by_section = doc.get_items_by_section([\n",
    "    \"Core Skills\",\n",
    "    \"Languages\",\n",
    "    \"Hardware\"\n",
    "])\n",
    "\n",
    "print(\"Items by section:\\n\")\n",
    "for section_name, items in items_by_section.items():\n",
    "    print(f\"{section_name}: {len(items)} items\")\n",
    "    print(f\"  {items}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case: Collect all skills from different sections\n",
    "all_skills = []\n",
    "skill_sections = [\"Core Skills\", \"Languages\", \"Hardware\"]\n",
    "items = doc.get_items_by_section(skill_sections)\n",
    "\n",
    "for section_items in items.values():\n",
    "    all_skills.extend(section_items)\n",
    "\n",
    "print(f\"Collected {len(all_skills)} total skills from {len(skill_sections)} sections\")\n",
    "print(f\"\\nFirst 10: {all_skills[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Text Property\n",
    "\n",
    "Each section has a `.text` property that formats structured data into **markdown** (not plaintext!).\n",
    "\n",
    "This is perfect for LLM consumption since markdown is very on-distribution for language models.\n",
    "\n",
    "**Key feature:** It's lazy-evaluated and cached!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a work_history section\n",
    "work_section = work_history_sections[0]\n",
    "\n",
    "# Check if text is cached\n",
    "print(f\"Text cached initially: {work_section._text_cache is not None}\")\n",
    "\n",
    "# Access text property (triggers formatting)\n",
    "text = work_section.text\n",
    "\n",
    "print(f\"Text cached after access: {work_section._text_cache is not None}\")\n",
    "print(f\"\\nMarkdown preview (first 700 chars):\\n\")\n",
    "print(text[:700])\n",
    "print(\"...\")\n",
    "\n",
    "# Show markdown structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Notice the markdown formatting:\")\n",
    "print(\"  - ## for section headers (e.g., 'Experience')\")\n",
    "print(\"  - ### for company names\")\n",
    "print(\"  - **bold** for job titles\")\n",
    "print(\"  - *italic* for dates\")\n",
    "print(\"  - #### for project headers\")\n",
    "print(\"  - - for bullet points\")\n",
    "print()\n",
    "print(\"NOTE: Some bullets may show malformed LaTeX like 'textbf'\")\n",
    "print(\"      This is a known bug documented in TODO.md\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Text\n",
    "\n",
    "Combine all sections into one searchable markdown document.\n",
    "\n",
    "**Page breaks:** The `---` markdown separator appears between pages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = doc.get_all_text()\n",
    "\n",
    "print(f\"Total text length: {len(all_text)} characters\")\n",
    "print(f\"Total words (approx): {len(all_text.split())}\")\n",
    "\n",
    "# Check page numbers\n",
    "page_numbers = sorted(set(s.page_number for s in doc.sections))\n",
    "print(f\"\\nPages in document: {page_numbers}\")\n",
    "\n",
    "# Count actual page breaks (not dates that use ---)\n",
    "page_break_count = all_text.count('\\n\\n---\\n\\n')\n",
    "print(f\"Page break markers: {page_break_count}\")\n",
    "\n",
    "print(f\"\\nSearching for 'Machine Learning'...\")\n",
    "# Simple text search\n",
    "count = all_text.lower().count('machine learning')\n",
    "print(f\"Found {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sections by page\n",
    "print(\"Sections by page:\\n\")\n",
    "for page in sorted(set(s.page_number for s in doc.sections)):\n",
    "    page_sections = [s for s in doc.sections if s.page_number == page]\n",
    "    print(f\"Page {page}:\")\n",
    "    for section in page_sections:\n",
    "        print(f\"  - {section.name} ({section.section_type})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ResumeDocumentArchive\n",
    "\n",
    "Load multiple resumes at once with error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize archive\n",
    "archive = ResumeDocumentArchive(RESUME_ARCHIVE_PATH)\n",
    "\n",
    "import warnings\n",
    "yaml_files = sorted(archive.structured_path.glob(\"*.yaml\"))\n",
    "\n",
    "i = 0\n",
    "errors = []\n",
    "for yaml_file in yaml_files:\n",
    "    try:\n",
    "        doc = ResumeDocument(yaml_file, mode=\"plaintext\")\n",
    "        i += 1\n",
    "    except Exception as e:\n",
    "        errors.append((yaml_file.name, str(e)))\n",
    "if errors:\n",
    "    error_summary = \"\\n\".join(f\"  - {name}: {error}\" for name, error in errors)\n",
    "    warnings.warn(\n",
    "        f\"Failed to load {len(errors)} YAML file(s):\\n{error_summary}\",\n",
    "        UserWarning\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pre-converted YAMLs\n",
    "documents = archive.load(mode=\"available\")\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nFirst 10 filenames:\")\n",
    "for doc in documents[:10]:\n",
    "    print(f\"  ‚Ä¢ {doc.filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Archive\n",
    "\n",
    "Now we can easily analyze all resumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_type_counts = Counter()\n",
    "\n",
    "for doc in documents:\n",
    "    for section in doc.sections:\n",
    "        section_type_counts[section.section_type] += 1\n",
    "\n",
    "print(\"Section types across all resumes:\")\n",
    "for stype, count in section_type_counts.most_common():\n",
    "    print(f\"  {stype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total work experiences across all work_history sections per resume\n",
    "work_exp_counts = []\n",
    "for doc in documents:\n",
    "    count = sum(\n",
    "        len(s.data.get('subsections', [])) \n",
    "        for s in doc.sections \n",
    "        if s.section_type == \"work_history\"\n",
    "    )\n",
    "    work_exp_counts.append(count)\n",
    "\n",
    "print(f\"Average work experiences per resume: {sum(work_exp_counts) / len(work_exp_counts):.1f}\")\n",
    "print(f\"Min: {min(work_exp_counts)}, Max: {max(work_exp_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching Across All Resumes\n",
    "\n",
    "Find resumes mentioning specific technologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_resumes_with_keyword(documents, keyword):\n",
    "    \"\"\"Find resumes containing a keyword.\"\"\"\n",
    "    matches = []\n",
    "    for doc in documents:\n",
    "        text = doc.get_all_text().lower()\n",
    "        if keyword.lower() in text:\n",
    "            matches.append(doc)\n",
    "    return matches\n",
    "\n",
    "# Search for different keywords\n",
    "keywords = [\"pytorch\", \"kubernetes\", \"llm\", \"hpc\", \"machine learning\", \"tensorflow\", \"docker\"]\n",
    "\n",
    "print(\"Keyword search results:\\n\")\n",
    "for keyword in keywords:\n",
    "    matches = find_resumes_with_keyword(documents, keyword)\n",
    "    print(f\"'{keyword}': {len(matches)} resumes ({len(matches)/len(documents)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: ResumeSection\n",
    "\n",
    "A peek under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ResumeDocument(sample_yaml)\n",
    "\n",
    "# ResumeSection is the basic building block\n",
    "section = doc.sections[0]  # Core Skills\n",
    "\n",
    "print(\"ResumeSection attributes:\")\n",
    "print(f\"  name: {section.name}\")\n",
    "print(f\"  section_type: {section.section_type}\")\n",
    "print(f\"  page_number: {section.page_number}\")\n",
    "print(f\"  region: {section.region}\")\n",
    "print(f\"  display_name: {section.display_name}\")\n",
    "print()\n",
    "\n",
    "# The data dict contains the actual content\n",
    "print(f\"Data structure for {section.section_type}:\")\n",
    "print(f\"  Keys: {list(section.data.keys())}\")\n",
    "print(f\"  Items: {len(section.data['items'])} skills\")\n",
    "print(f\"  First 3: {section.data['items'][:3]}\")\n",
    "print()\n",
    "\n",
    "# Compare with a wrapper section\n",
    "wrapper = doc.sections[3]  # ML Infrastructure Tools\n",
    "print(f\"Data structure for {wrapper.section_type}:\")\n",
    "print(f\"  Keys: {list(wrapper.data.keys())}\")\n",
    "print(f\"  Subsections: {len(wrapper.data['subsections'])}\")\n",
    "print(f\"  First subsection: {wrapper.data['subsections'][0]['name']}\")\n",
    "print(f\"    Type: {wrapper.data['subsections'][0]['type']}\")\n",
    "print(f\"    Items: {len(wrapper.data['subsections'][0]['items'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section types follow consistent patterns:\n",
    "\n",
    "print(\"Pattern 1: Direct content sections (items at top level)\")\n",
    "print(\"  Types: skill_list_caps, skill_list_pipes, personality_alias_array\")\n",
    "print(\"  Structure: {'items': [...], 'name': '...'}\")\n",
    "print()\n",
    "\n",
    "direct_section = doc.get_section(\"Core Skills\")\n",
    "print(f\"  Example: {direct_section.name}\")\n",
    "print(f\"    data.keys(): {list(direct_section.data.keys())}\")\n",
    "print(f\"    items[0]: {direct_section.data['items'][0]}\")\n",
    "print()\n",
    "\n",
    "print(\"Pattern 2: Wrapper sections (subsections with items)\")\n",
    "print(\"  Types: skill_categories, projects\")\n",
    "print(\"  Structure: {'subsections': [{'type': '...', 'name': '...', 'items': [...]}, ...]}\")\n",
    "print()\n",
    "\n",
    "wrapper_section = doc.get_section(\"ML Infrastructure Tools\")\n",
    "print(f\"  Example: {wrapper_section.name}\")\n",
    "print(f\"    data.keys(): {list(wrapper_section.data.keys())}\")\n",
    "print(f\"    subsections[0]: {wrapper_section.data['subsections'][0]['name']}\")\n",
    "print(f\"      type: {wrapper_section.data['subsections'][0]['type']}\")\n",
    "print(f\"      items: {wrapper_section.data['subsections'][0]['items'][:2]}\")\n",
    "print()\n",
    "\n",
    "print(\"Pattern 3: Work history (flattened work_experience)\")\n",
    "print(\"  Type: work_history\")\n",
    "print(\"  Structure: {'subsections': [{'type': 'work_experience', 'company': '...', 'title': '...', 'items': [...], 'projects': [...]}]}\")\n",
    "print()\n",
    "\n",
    "work_section = doc.get_section(\"Experience\")\n",
    "print(f\"  Example: {work_section.name}\")\n",
    "print(f\"    data.keys(): {list(work_section.data.keys())}\")\n",
    "print(f\"    subsections[0]['company']: {work_section.data['subsections'][0]['company']}\")\n",
    "print(f\"    subsections[0].keys(): {list(work_section.data['subsections'][0].keys())}\")\n",
    "print(f\"    Has 'items': {('items' in work_section.data['subsections'][0])}\")\n",
    "print(f\"    Has 'projects': {('projects' in work_section.data['subsections'][0])}\")\n",
    "if work_section.data['subsections'][0]['projects']:\n",
    "    print(f\"    First project: {work_section.data['subsections'][0]['projects'][0]['name']}\")\n",
    "    print(f\"      project type: {work_section.data['subsections'][0]['projects'][0]['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Key Advantages Summary\n",
    "\n",
    "### OmegaConf\n",
    "‚ùå Complex nested navigation  \n",
    "‚ùå Mixed with formatting metadata  \n",
    "‚ùå LaTeX commands in text  \n",
    "‚ùå Boilerplate code for every operation  \n",
    "‚ùå Hard to search/analyze  \n",
    "\n",
    "### ResumeDocument\n",
    "‚úÖ Flat, direct section access  \n",
    "‚úÖ Content only (no formatting)  \n",
    "‚úÖ **Markdown output** (great for LLM comprehension)  \n",
    "‚úÖ LaTeX automatically converted to markdown  \n",
    "‚úÖ **Page breaks preserved** (`---` separators between pages)  \n",
    "‚úÖ **Professional profile included** at the top  \n",
    "‚úÖ **Table of contents property** for quick overview  \n",
    "‚úÖ **Region tracking** (left column vs main column)  \n",
    "‚úÖ **Hierarchical structure preserved** (work_history contains multiple work experiences)  \n",
    "‚úÖ Simple, reusable API  \n",
    "‚úÖ Easy search and analysis  \n",
    "‚úÖ Batch operations with error handling  \n",
    "\n",
    "### Markdown Formatting\n",
    "\n",
    "The `.text` property returns markdown with:\n",
    "- `#` header for professional profile (name | brand)\n",
    "- `##` headers for section headings (e.g., \"Experience\", \"Education\")\n",
    "- `###` headers for companies/institutions\n",
    "- `####` headers for projects\n",
    "- **Bold** for emphasis (from `\\textbf{}`, `\\coloremph{}`)\n",
    "- *Italic* usually for dates (from `\\textit{}`)\n",
    "- `-` for bullet points\n",
    "- `` `code` `` for `\\texttt{}` (usually software names)\n",
    "- `---` page breaks between pages\n",
    "\n",
    "This hierarchy prevents duplicate section headers and groups related work experiences together!\n",
    "\n",
    "This is *on-distribution* for LLMs, making it perfect for:\n",
    "- Prompting language models\n",
    "- Semantic search with embeddings\n",
    "- Content relevance scoring\n",
    "- Automated analysis\n",
    "\n",
    "### Perfect for Targeting Context ‚úÖ\n",
    "\n",
    "The Targeting context can now:\n",
    "- Get quick resume overview with `doc.table_of_contents`\n",
    "- Search historical resumes for relevant content\n",
    "- Score sections by relevance to job description\n",
    "- Find similar past applications\n",
    "- Extract bullets matching specific criteria\n",
    "- Pass markdown directly to LLMs for decision-making\n",
    "- Preserve page structure for layout decisions\n",
    "- Track which sections are in left vs main column\n",
    "- Access grouped work experiences within work_history sections\n",
    "- All without worrying about LaTeX formatting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
